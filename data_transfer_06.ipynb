{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFER ISA STRUCTURE FROM ONE SEEK TO ANOTHER: ASSAY AND DATA_FILE (INCLUDING BLOB)\n",
    "\n",
    "# SOURCE SEEK: \n",
    "#    GET/READ/PRINT JSON RESOURCE (ASSAY)    readJsonData()               (session.get)\n",
    "#    PRINT RELATIONSHIPS OF AN ASSAY         printJsonDataRelationships()  \n",
    "#    READ DATA FILE BLOB, DOWNLOAD BLOB      readBlobData()(              (session.get, urlopen(Request(url=download_link, headers=headers2)))\n",
    "#    ISA STRUCTURE (ASSAY)                   determineISAstructureFromRelationships()  (not really doing anything)\n",
    "# TARGET SEEK: \n",
    "#    REGISTER ASSAY                          registerAssay()              (session.post)\n",
    "#    REGISTER DATA FILE AND BLOB             registerBlobData()           (session.post)\n",
    "#    UPLOAD BLOB INTO DATA FILE              uploadBlobData()             (session.put)\n",
    "#    COMBINES REGISTER DATA FILE AND UPLOAD BLOB   TransferData()\n",
    "\n",
    "# USING 2 SEEKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import the libraries so that they can be used within the notebook\n",
    "\n",
    "  * **requests** is used to make HTTP calls\n",
    "  * **json** is used to encode and decode strings into JSON\n",
    "  * **string** is used to perform text manipulation and checking\n",
    "  * **pandas** helps format the JSON data in a more readable format\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import string\n",
    "# Importing the libraries we need to format the data in a more readable way. \n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "#authentication\n",
    "import getpass\n",
    "import urllib.request\n",
    "from urllib.request import urlopen, Request\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS\n",
    "\n",
    "def authenticate(headers):\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers)\n",
    "    session.auth = (input('Username:'), getpass.getpass('Password')) \n",
    "    return session\n",
    "\n",
    "### GET JSON\n",
    "def json_for_resource(session, headers_json, url, type, id):    \n",
    "  r = session.get(url + \"/\" + type + \"/\" + str(id), headers=headers_json)\n",
    "  if (r.status_code != 200):\n",
    "    print(r.json())\n",
    "  r.raise_for_status()\n",
    "  return r.json()\n",
    "\n",
    "### READ / PRINT JSON\n",
    "def readJsonData(session, headers_json, url, data_id, data_type):\n",
    "    result_json = json_for_resource(session, headers_json, url, data_type, data_id)\n",
    "    filetitle = result_json['data']['attributes']['title']\n",
    "    print(\"Name of \\'\" + data_type + \"\\': \" + filetitle + \"\\n\")\n",
    "    print(result_json)\n",
    "    return result_json\n",
    "\n",
    "### PRINT RELATIONSHIPS OF A JSON\n",
    "def printJsonDataRelationships(session, headers_json, source_base_url, input_data):#, data_types_list\n",
    "    files = []\n",
    "    #for dtype in data_types_list:\n",
    "    for dtype in input_data['data']['relationships']:\n",
    "        print(dtype)    #data_files, investigation, study, projects\n",
    "\n",
    "        if(dtype=='investigation' or dtype=='study'):#formated differently?\n",
    "            #'projects': {'data': [{'id': '2', 'type': 'projects'}]}\n",
    "            #'investigation': {'data': {'id': '3', 'type': 'investigations'}}\n",
    "            #'study': {'data': {'id': '3', 'type': 'studies'}}\n",
    "            #'data_files': {'data': [{'id': '38', 'type': 'data_files'}]}\n",
    "            item = input_data['data']['relationships'][str(dtype)]['data']\n",
    "            #print(\"item \", item)\n",
    "\n",
    "            j = json_for_resource(session,headers_json,source_base_url,item['type'],item['id'])  \n",
    "            files.append({\n",
    "                'type':j['data']['type'],\n",
    "                'id':j['data']['id'],\n",
    "                'title':j['data']['attributes']['title'],      \n",
    "            })\n",
    "\n",
    "        else: #if(dtype=='data_files' or dtype=='projects'):\n",
    "            for item in input_data['data']['relationships'][str(dtype)]['data']:\n",
    "                #print(\"item \", item)\n",
    "\n",
    "                j = json_for_resource(session,headers_json,source_base_url,item['type'],item['id'])  \n",
    "                files.append({\n",
    "                    'type':str(dtype),\n",
    "                    'id':j['data']['id'],\n",
    "                    'title':j['data']['attributes']['title'],      \n",
    "                })\n",
    "\n",
    "    print() \n",
    "    print(str(len(files)) + \" relationships found: \\n\") #print(str(len(files)) + \" \\'\" + grep_typep + \"\\' found: \\n\") \n",
    "    print(json_normalize(files)) \n",
    "    return files\n",
    "\n",
    "\n",
    "### ISA STRUCTURE (ASSAY) (not really doing anything)\n",
    "def determineISAstructureFromRelationships(input_relationships):#only for assays, single data file\n",
    "    \n",
    "    #print(source_relationships)\n",
    "    #print(source_relationships[0]['type'])\n",
    "    #print(len(source_relationships))\n",
    "    \n",
    "    if(source_data_type == 'assays'):\n",
    "        structure = ['projects', 'investigations', 'studies', 'data_files', 'creators']\n",
    "        isa_structure = []\n",
    "    #    for x in range(0, len(source_relationships)):\n",
    "    #        #print(source_relationships[x]['type'])\n",
    "    #        dtype = source_relationships[x]['type']\n",
    "    #        #print(dtype)\n",
    "    #        if(dtype=='projects'):\n",
    "    #            isa_structure.append(source_relationships[x])\n",
    "    #            \n",
    "    #    for x in range(0, len(source_relationships)):\n",
    "    #        #print(source_relationships[x]['type'])\n",
    "    #        dtype = source_relationships[x]['type']\n",
    "    #        #print(dtype)\n",
    "    #        if(dtype=='investigations'):\n",
    "    #            isa_structure.append(source_relationships[x])\n",
    "    #            \n",
    "    #    for x in range(0, len(source_relationships)):\n",
    "    #        #print(source_relationships[x]['type'])\n",
    "    #        dtype = source_relationships[x]['type']\n",
    "    #        #print(dtype)\n",
    "    #        if(dtype=='studies'):\n",
    "    #            isa_structure.append(source_relationships[x])\n",
    "\n",
    "        for y in range(0, len(structure)):\n",
    "            for x in range(0, len(input_relationships)):\n",
    "                dtype = input_relationships[x]['type']\n",
    "                if(dtype==structure[y]):\n",
    "                    isa_structure.append(input_relationships[x])\n",
    "\n",
    "    #print()\n",
    "    print(json_normalize(isa_structure))\n",
    "    return isa_structure\n",
    "\n",
    "\n",
    "### READ DATA_FILE DATA, GET BLOB DATA\n",
    "def readBlobData(session, headers_json, headers_token, url, data_id, data_type):\n",
    "    result_json = json_for_resource(session, headers_json, url, data_type, data_id)#uses session\n",
    "    \n",
    "    filetitle = result_json['data']['attributes']['title']\n",
    "    #print(\"Name of \\'\" + data_type + \"\\': \" + filetitle + \"\\n\")\n",
    "    #print(\"Policy: \", result_json['data']['attributes']['policy'],\"\\n\")\n",
    "    filelicense = result_json['data']['attributes']['license']\n",
    "    \n",
    "    blob = result_json['data']['attributes']['content_blobs'][0]\n",
    "    #print(\"Blob: \", blob,\"\\n\")\n",
    "    \n",
    "    filename = blob['original_filename']\n",
    "    filetype = blob['content_type']\n",
    "\n",
    "    \n",
    "    link = blob['link']\n",
    "    download_link = link + \"/download\"\n",
    "    #print(\"Download link is: \" + download_link)\n",
    "    \n",
    "    #get blob data\n",
    "    #response = urllib.request.urlopen(download_link)\n",
    "    ###from urllib.request import urlopen, Request\n",
    "    req = Request(url=download_link, headers=headers_token) \n",
    "    data = urlopen(req).read()\n",
    "    \n",
    "    #data = response.read()\n",
    "    #print(response)\n",
    "    #print(data)\n",
    "    return result_json, filetitle, filename, filetype, filelicense, link, download_link, data\n",
    "\n",
    "\n",
    "### REGISTER ASSAY\n",
    "def registerAssay(session, in_assay_json, target_project_id, target_investigation_id, target_study_id, target_creator_id):\n",
    "    new_assay_json = {}\n",
    "    new_assay_json['data'] = {}\n",
    "    new_assay_json['data']['type'] = 'assays'\n",
    "\n",
    "    new_assay_json['data']['attributes'] = {}\n",
    "    new_assay_json['data']['attributes']['title'] = in_assay_json['data']['attributes']['title']\n",
    "    new_assay_json['data']['attributes']['description'] = in_assay_json['data']['attributes']['description']\n",
    "\n",
    "    #new_assay_json['data']['attributes']['policy'] = assay_json['data']['attributes']['policy']\n",
    "    new_assay_json['data']['attributes']['policy'] = {'access':'no_access'}\n",
    "    new_assay_json['data']['attributes']['policy']['permissions'] = [{'resource':{'id':target_project_id,'type':'projects'},'access':'download'}];\n",
    "\n",
    "    new_assay_json['data']['attributes']['assay_class'] = in_assay_json['data']['attributes']['assay_class']\n",
    "    new_assay_json['data']['attributes']['assay_type'] = in_assay_json['data']['attributes']['assay_type']\n",
    "    new_assay_json['data']['attributes']['technology_type'] = in_assay_json['data']['attributes']['technology_type']\n",
    "\n",
    "    new_assay_json['data']['relationships'] = {}\n",
    "    new_assay_json['data']['relationships']['creators'] = {}\n",
    "    new_assay_json['data']['relationships']['creators']['data'] = [{'id' : target_creator_id, 'type' : 'people'}]\n",
    "    new_assay_json['data']['relationships']['study'] = {}\n",
    "    new_assay_json['data']['relationships']['study']['data'] = {'id' : target_study_id, 'type' : 'studies'}\n",
    "    new_assay_json['data']['relationships']['investigation'] = {}\n",
    "    new_assay_json['data']['relationships']['investigation']['data'] = {'id' : target_investigation_id, 'type' : 'investigations'}\n",
    "    new_assay_json['data']['relationships']['projects'] = {}\n",
    "    new_assay_json['data']['relationships']['projects']['data'] = {'id' : target_project_id, 'type' : 'projects'}\n",
    "\n",
    "    r = session.post(target_base_url + '/assays', json=new_assay_json)\n",
    "    r.raise_for_status()\n",
    "    populated_assay = r.json()\n",
    "    print(\"Registered assay: \", populated_assay)   \n",
    "    assay_id = populated_assay['data']['id']\n",
    "    \n",
    "    return assay_id\n",
    "\n",
    "\n",
    "### REGISTER DATA FILE AND BLOB \n",
    "def registerBlobData(session, base_url, data_type, filetitle, filelicense, blob, target_project_id, target_investigation_id, target_study_id, target_assay_id, target_creator_id):\n",
    "    data_array_name = {}\n",
    "    data_array_name['data'] = {}\n",
    "    data_array_name['data']['type'] = data_type\n",
    "    \n",
    "    data_array_name['data']['attributes'] = {}\n",
    "    data_array_name['data']['attributes']['title'] = filetitle\n",
    "    data_array_name['data']['attributes']['license'] = filelicense #'CC-BY-4.0'\n",
    "    #data_array_name['data']['attributes']['policy'] = {'access':'download'}\n",
    "    data_array_name['data']['attributes']['policy'] = {'access':'no_access'}\n",
    "    data_array_name['data']['attributes']['policy']['permissions'] = [{'resource':{'id':target_project_id,'type':'projects'},'access':'download'}];\n",
    "    data_array_name['data']['attributes']['content_blobs'] = [blob] #error if blob is not there\n",
    "        \n",
    "    data_array_name['data']['relationships'] = {}\n",
    "    data_array_name['data']['relationships']['projects'] = {}\n",
    "    data_array_name['data']['relationships']['projects']['data'] = [{'id' : target_project_id, 'type' : 'projects'}]\n",
    "    data_array_name['data']['relationships']['investigations'] = {}\n",
    "    data_array_name['data']['relationships']['investigations']['data'] = [{'id' : target_investigation_id, 'type' : 'investigations'}]\n",
    "    data_array_name['data']['relationships']['studies'] = {}\n",
    "    data_array_name['data']['relationships']['studies']['data'] = [{'id' : target_study_id, 'type' : 'studies'}]\n",
    "    data_array_name['data']['relationships']['assays'] = {}\n",
    "    data_array_name['data']['relationships']['assays']['data'] = [{'id' : target_assay_id, 'type' : 'assays'}]\n",
    "    data_array_name['data']['relationships']['creators'] = {}\n",
    "    data_array_name['data']['relationships']['creators']['data'] = [{'id' : target_creator_id, 'type' : 'people'}]\n",
    "    \n",
    "    #register data file\n",
    "    r = session.post(base_url + '/' + data_type, json = data_array_name)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    populated_data_file = r.json()\n",
    "    print(\"Registered data_file: \", populated_data_file[\"data\"])\n",
    "    #print(\"Registered json:\")\n",
    "    data_file_id = populated_data_file[\"data\"]['id']\n",
    "    data_file_link = populated_data_file['data']['attributes']['content_blobs'][0]['link']  \n",
    "\n",
    "    \n",
    "    return data_file_id, data_file_link\n",
    "\n",
    "\n",
    "### UPLOAD BLOB INTO DATA FILE\n",
    "def uploadBlobData(session, headers_json, headers_stream, base_url, data_type, blob_id, blob_url, binary_data):\n",
    "\n",
    "    #get url from json content blob\n",
    "    #blob_url = registered_json_data['data']['attributes']['content_blobs'][0]['link']    \n",
    " \n",
    "    #PUT data\n",
    "    upload = session.put(blob_url, data = binary_data, headers = headers_stream)\n",
    "    upload.raise_for_status()\n",
    "    \n",
    "    #print content blob\n",
    "    #blob_id = registered_json_data['data']['id']  \n",
    "    created_json = json_for_resource(session, headers_json, base_url, data_type, blob_id)\n",
    "    print(\"Uploaded blob data: \", created_json['data']['attributes']['content_blobs'])\n",
    "    \n",
    "        \n",
    "### COMBINES REGISTER DATA FILE AND UPLOAD BLOB (not needed)\n",
    "def TransferData(session, headers_json, headers_stream, base_url, data_type, filetitle, filelicense, blob, dataBinary,\n",
    "    target_project_id, target_investigation_id, target_study_id, target_assay_id, target_creator_id): # register, upload\n",
    "    #registered_json_data = registerBlobData(session, base_url, data_type, filetitle, blob)\n",
    "    #uploadBlobData(session, base_url, data_type, registered_json_data, dataBinary)\n",
    "    \n",
    "    #target_data_file  = registerBlobData(\n",
    "    #    session2, target_base_url, target_data_file_data_type, target_filetitle, target_filelicense, target_blob, \n",
    "    #    target_project_id, target_investigation_id, target_study_id, target_assay_id, target_creator_id2)\n",
    "    #target_data_file_id = target_data_file[0]\n",
    "    #target_data_file_link = target_data_file[1]\n",
    "    \n",
    "    #uploadBlobData(session2, headers3, target_base_url, target_data_file_data_type, target_data_file_id, target_data_file_link, dataBinary)\n",
    "    \n",
    "    target_data_file  = registerBlobData(\n",
    "        session, base_url, data_type, filetitle, filelicense, blob, \n",
    "        target_project_id, target_investigation_id, target_study_id, target_assay_id, target_creator_id)\n",
    "    target_data_file_id = target_data_file[0]\n",
    "    target_data_file_link = target_data_file[1]\n",
    "    \n",
    "    print()\n",
    "    uploadBlobData(session, headers_json, headers_stream, base_url, data_type, target_data_file_id, target_data_file_link, dataBinary)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AUTHENTICATION\n",
    "headers1 = {\"Accept\": \"application/vnd.api+json\", \"Accept-Charset\": \"ISO-8859-1\"} #headers_json\n",
    "\n",
    "API_TOKEN = open(\"token\").readline().strip() #\"user:password\" encoded in base64\n",
    "headers2 = { #headers_token\n",
    "       \"Authorization\": \"Basic %s\" %API_TOKEN,\n",
    "       'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "\n",
    "headers3 = {'Content-Type': 'application/octet-stream'} #headers_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:dudasda\n",
      "Password········\n"
     ]
    }
   ],
   "source": [
    "session1 = authenticate(headers1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:dudasda\n",
      "Password········\n"
     ]
    }
   ],
   "source": [
    "session2 = authenticate(headers1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOURCE DATA PARAMETERS\n",
    "\n",
    "source_base_url = 'http://localhost:3000'\n",
    "\n",
    "### assay to be copied\n",
    "source_assay_id = 4\n",
    "source_data_type = 'assays'\n",
    "\n",
    "### data: network image\n",
    "#source_data_id = 38 # 22                 \n",
    "#source_data_type = 'data_files'\n",
    "\n",
    "### upwards structure of the data: 1 project \n",
    "#source_project_id = 1      # Default Project\n",
    "##source_project_id = 2      # Project Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of 'assays': Assay to be copied\n",
      "\n",
      "{'data': {'id': '4', 'type': 'assays', 'attributes': {'policy': {'access': 'no_access', 'permissions': [{'resource': {'id': '2', 'type': 'projects'}, 'access': 'download'}]}, 'title': 'Assay to be copied', 'description': 'Description of an assay to be copied.', 'other_creators': None, 'assay_class': {'title': 'Experimental assay', 'key': 'EXP', 'description': None}, 'assay_type': {'label': 'Experimental Assay Type', 'uri': 'http://jermontology.org/ontology/JERMOntology#Experimental_assay_type'}, 'technology_type': {'label': 'Technology Type', 'uri': 'http://jermontology.org/ontology/JERMOntology#Technology_type'}, 'tags': None}, 'relationships': {'creators': {'data': [{'id': '1', 'type': 'people'}]}, 'submitter': {'data': [{'id': '1', 'type': 'people'}]}, 'organisms': {'data': []}, 'people': {'data': [{'id': '1', 'type': 'people'}]}, 'projects': {'data': [{'id': '2', 'type': 'projects'}]}, 'investigation': {'data': {'id': '3', 'type': 'investigations'}}, 'study': {'data': {'id': '3', 'type': 'studies'}}, 'data_files': {'data': [{'id': '38', 'type': 'data_files'}]}, 'models': {'data': []}, 'sops': {'data': []}, 'publications': {'data': []}, 'documents': {'data': []}}, 'links': {'self': '/assays/4'}, 'meta': {'created': '2019-04-08T10:50:31.000Z', 'modified': '2019-04-08T10:50:31.000Z', 'api_version': '0.1', 'uuid': '0d632790-3c1a-0137-b350-10e7c619e7b6', 'base_url': 'http://localhost:3000'}}, 'jsonapi': {'version': '1.0'}}\n"
     ]
    }
   ],
   "source": [
    "### GET/READ/PRINT JSON RESOURCE (ASSAY)\n",
    "assay_json = readJsonData(session1, headers1, source_base_url, source_assay_id, source_data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creators\n",
      "submitter\n",
      "organisms\n",
      "people\n",
      "projects\n",
      "investigation\n",
      "study\n",
      "data_files\n",
      "models\n",
      "sops\n",
      "publications\n",
      "documents\n",
      "\n",
      "7 relationships found: \n",
      "\n",
      "   id              title            type\n",
      "0   1      Dorotea Dudas        creators\n",
      "1   1      Dorotea Dudas       submitter\n",
      "2   1      Dorotea Dudas          people\n",
      "3   2      Project Alpha        projects\n",
      "4   3  investigation one  investigations\n",
      "5   3          study one         studies\n",
      "6  38      Network Image      data_files\n"
     ]
    }
   ],
   "source": [
    "### PRINT RELATIONSHIPS OF A JSON\n",
    "#grep_type = ['data_files', 'investigation', 'study', 'projects','creators','submitter','people']\n",
    "source_relationships = printJsonDataRelationships(session1, headers1, source_base_url, assay_json)#, grep_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type:\t assays \n",
      "File Type:\t image/png \n",
      "File License:\t CC-BY-4.0\n",
      "File Name:\t image_02.png \n",
      "File Title:\t Network Image \n",
      "Download link:\t http://localhost:3000/data_files/38/content_blobs/41/download\n",
      "Binary Data:\t b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x02X\\x00\\x00\\x01\\x90\\x08\\x06\\x00\\x00\\x00r'  etc.\n"
     ]
    }
   ],
   "source": [
    "### READ DATA FILE BLOB FROM SOURCE\n",
    "\n",
    "###data file id(s) (could be read automatically)\n",
    "#data_file_id = 38\n",
    "data_file_id = source_relationships[6]['id']\n",
    "\n",
    "#return result_json, filetitle, filename, filetype, filelicense, link, download_link, data\n",
    "dataRead = readBlobData(session1, headers1, headers2, source_base_url, data_file_id, 'data_files')\n",
    "print(\"Data Type:\\t\", source_data_type, \"\\nFile Type:\\t\", dataRead[3], \"\\nFile License:\\t\", dataRead[4])\n",
    "print(\"File Name:\\t\", dataRead[2], \"\\nFile Title:\\t\", dataRead[1], \"\\nDownload link:\\t\", dataRead[6])\n",
    "print(\"Binary Data:\\t\", dataRead[7][0:30], \" etc.\")\n",
    "\n",
    "#data to be uploaded\n",
    "dataBinary = dataRead[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id              title            type\n",
      "0   2      Project Alpha        projects\n",
      "1   3  investigation one  investigations\n",
      "2   3          study one         studies\n",
      "3  38      Network Image      data_files\n",
      "4   1      Dorotea Dudas        creators\n"
     ]
    }
   ],
   "source": [
    "### ISA STRUCTURE SOURCE (ASSAY) (only for assays, single data file) (not really doing anything)\n",
    "isas = determineISAstructureFromRelationships(source_relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TARGET DATA PARAMETERS\n",
    "### need to: register assay (get assay id), register data_file (get data_file id), upload blob\n",
    "\n",
    "### DATABASE\n",
    "#target_base_url = 'http://localhost:4000'\n",
    "target_base_url = 'http://doroteadesktop:4000'\n",
    "\n",
    "### ASSAY: register assay\n",
    "#target_assay_id - will be obtained after registering an assay\n",
    "target_assay_data_type = source_data_type #'assays'\n",
    "\n",
    "\n",
    "##### ISA STRUCTURE (need to get it from Doro2Dom)\n",
    "\n",
    "### UPWARDS STRUCTURE of the assay: 1 project, 1 investigation, 1 study \n",
    "#target_project_id = 1      # Default Project   (same id as source seek, since they are originaly clones)\n",
    "target_project_id = 2       # Project Alpha     (same id as source seek, since they are originaly clones)\n",
    "target_investigation_id = 3 # investigation one (same id as source seek, since they are originaly clones)\n",
    "#target_study_id = 3        # study one         (same id as source seek, since they are originaly clones)\n",
    "target_study_id = 4         # study two         (made just for this purpose, not in source seek)\n",
    "target_creator_id = 1      # Dorotea           (same id as source seek, since they are originaly clones)\n",
    "target_creator_id2 = 3      # Teodora           (same id as source seek, since they are originaly clones)\n",
    "\n",
    "### DOWNWARDS STRUCTURE of the assay: 1 data file\n",
    "#target_data_file_id             - will be obtained after registering a data_file\n",
    "target_data_file_data_type = 'data_files'\n",
    "target_filetitle = dataRead[1] + ' from DoroDom to Doro2Dom together with an Assay 3'\n",
    "target_filelicense = dataRead[4]\n",
    "# data_file blob\n",
    "target_filename = dataRead[2]\n",
    "target_filetype = dataRead[3]\n",
    "target_blob = {'original_filename' : target_filename, 'content_type' : target_filetype}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered assay:  {'data': {'id': '14', 'type': 'assays', 'attributes': {'policy': {'access': 'no_access', 'permissions': [{'resource': {'id': '2', 'type': 'projects'}, 'access': 'download'}]}, 'title': 'Assay to be copied', 'description': 'Description of an assay to be copied.', 'other_creators': None, 'assay_class': {'title': 'Experimental assay', 'key': 'EXP', 'description': None}, 'assay_type': {'label': 'Experimental Assay Type', 'uri': 'http://jermontology.org/ontology/JERMOntology#Experimental_assay_type'}, 'technology_type': {'label': 'Technology Type', 'uri': 'http://jermontology.org/ontology/JERMOntology#Technology_type'}, 'tags': None}, 'relationships': {'creators': {'data': [{'id': '1', 'type': 'people'}]}, 'submitter': {'data': [{'id': '1', 'type': 'people'}]}, 'organisms': {'data': []}, 'people': {'data': [{'id': '1', 'type': 'people'}]}, 'projects': {'data': [{'id': '2', 'type': 'projects'}]}, 'investigation': {'data': {'id': '3', 'type': 'investigations'}}, 'study': {'data': {'id': '4', 'type': 'studies'}}, 'data_files': {'data': []}, 'models': {'data': []}, 'sops': {'data': []}, 'publications': {'data': []}, 'documents': {'data': []}}, 'links': {'self': '/assays/14'}, 'meta': {'created': '2019-04-09T14:17:40.860Z', 'modified': '2019-04-09T14:17:40.832Z', 'api_version': '0.1', 'uuid': '2888b9d0-3d00-0137-b351-10e7c619e7b6', 'base_url': 'http://localhost:4000'}}, 'jsonapi': {'version': '1.0'}}\n",
      "\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "### REGISTER ASSAY (in second seek and get the id) (TO THE STUDY IN THE 2nd SEEK (RESPECTING THE UPWARDS STRUCTURE))\n",
    "\n",
    "target_assay_id = registerAssay(session2, assay_json, target_project_id, target_investigation_id, target_study_id, target_creator_id)\n",
    "print()\n",
    "print(target_assay_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered data_file:  {'id': '55', 'type': 'data_files', 'attributes': {'policy': {'access': 'no_access', 'permissions': [{'resource': {'id': '2', 'type': 'projects'}, 'access': 'download'}]}, 'title': 'Network Image from DoroDom to Doro2Dom together with an Assay 3', 'description': None, 'license': 'CC-BY-4.0', 'latest_version': 1, 'tags': None, 'versions': [{'version': 1, 'revision_comments': None, 'url': 'http://localhost:4000/data_files/55?version=1'}], 'version': 1, 'revision_comments': None, 'created_at': '2019-04-09T14:17:44.000Z', 'updated_at': '2019-04-09T14:17:44.000Z', 'content_blobs': [{'original_filename': 'image_02.png', 'url': None, 'md5sum': None, 'sha1sum': None, 'content_type': 'image/png', 'link': 'http://localhost:4000/data_files/55/content_blobs/58', 'size': None}], 'other_creators': None}, 'relationships': {'creators': {'data': [{'id': '3', 'type': 'people'}]}, 'submitter': {'data': [{'id': '1', 'type': 'people'}]}, 'people': {'data': [{'id': '1', 'type': 'people'}, {'id': '3', 'type': 'people'}]}, 'projects': {'data': [{'id': '2', 'type': 'projects'}]}, 'investigations': {'data': [{'id': '3', 'type': 'investigations'}]}, 'studies': {'data': [{'id': '4', 'type': 'studies'}]}, 'assays': {'data': [{'id': '14', 'type': 'assays'}]}, 'publications': {'data': []}, 'events': {'data': []}}, 'links': {'self': '/data_files/55?version=1'}, 'meta': {'created': '2019-04-09T14:17:43.995Z', 'modified': '2019-04-09T14:17:43.972Z', 'api_version': '0.1', 'uuid': '2a6839b0-3d00-0137-b351-10e7c619e7b6', 'base_url': 'http://localhost:4000'}}\n",
      "\n",
      "55 http://localhost:4000/data_files/55/content_blobs/58\n"
     ]
    }
   ],
   "source": [
    "### REGISTER DATA FILE AND BLOB \n",
    "\n",
    "target_data_file  = registerBlobData(\n",
    "    session2, target_base_url, target_data_file_data_type, target_filetitle, target_filelicense, target_blob, \n",
    "    target_project_id, target_investigation_id, target_study_id, target_assay_id, target_creator_id2)\n",
    "target_data_file_id = target_data_file[0]\n",
    "target_data_file_link = target_data_file[1]\n",
    "print()\n",
    "print(target_data_file_id, target_data_file_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded blob data:  [{'original_filename': 'image_02.png', 'url': None, 'md5sum': '8b614ae8a242fe8419099b6f77a9c9e5', 'sha1sum': '2898b02b0dfd0dd6615c555ec15ba0744a08b9ef', 'content_type': 'image/png', 'link': 'http://localhost:4000/data_files/55/content_blobs/58', 'size': 140675}]\n"
     ]
    }
   ],
   "source": [
    "### UPLOAD BLOB INTO DATA FILE IN TARGET DATABASE\n",
    "\n",
    "uploadBlobData(session2, headers1, headers3, target_base_url, target_data_file_data_type, target_data_file_id, target_data_file_link, dataBinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### COMBINES REGISTER DATA FILE AND UPLOAD BLOB\n",
    "\n",
    "#TransferData(session2, headers1, headers3, target_base_url, target_data_file_data_type, \n",
    "#             target_filetitle, target_filelicense, target_blob, dataBinary, \n",
    "#             target_project_id, target_investigation_id, target_study_id, target_assay_id, target_creator_id2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Close the HTTP **session**\"\"\"\n",
    "session.close()\n",
    "session2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DELETE\n",
    "\n",
    "#assay_url = populated_assay['data']['links']['self']\n",
    "#study_url = populated_study['data']['links']['self']\n",
    "#investigation_url = populated_investigation['data']['links']['self']\n",
    "\n",
    "#session.delete(base_url + assay_url)\n",
    "#session.delete(base_url + study_url)\n",
    "#session.delete(base_url + investigation_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
